import os

MODEL_CONFIG = {
    "base_url": "http://localhost:11434",
    "model": "llama3.1",
    "temperature": 0.8,
    "keep_alive": 500,
}


EMBEDDING_PATH = "data/processed/"

DEBUG_FLAG = False


LLM_CACHE_PATH = "data/cache/"
